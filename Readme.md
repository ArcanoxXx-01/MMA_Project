# MMA Project â€“ Evaluador AutomÃ¡tico de Respuestas AcadÃ©micas

## ğŸ§© DefiniciÃ³n del problema a resolver

El proyecto busca facilitar la evaluaciÃ³n de respuestas acadÃ©micas abiertas (por ejemplo, preguntas teÃ³ricas o conceptuales) que tradicionalmente requieren tiempo y criterio subjetivo por parte de los profesores. Los objetivos principales son:

- Reducir la carga de trabajo en la correcciÃ³n de respuestas escritas.
- Aumentar la objetividad y consistencia en las evaluaciones.
- Ofrecer retroalimentaciÃ³n automÃ¡tica y rÃ¡pida al estudiante.

---

### ğŸ’¡ Propuesta de soluciÃ³n

Se diseÃ±Ã³ una aplicaciÃ³n web dividida en dos mÃ³dulos principales:

1. **Backend** que gestiona problemas acadÃ©micos y usa modelos de lenguaje (LLMs) para evaluar las respuestas.
2. **Frontend** que permite a los usuarios (profesores) gestionar preguntas, filtrar por criterios, y visualizar resultados.

La evaluaciÃ³n automÃ¡tica se realiza a travÃ©s de un LLM instructivo (modelo chat) de Fireworks.ai, con prompts diseÃ±ados para evaluar la respuesta del estudiante en base al tema, tipo de pregunta, soluciÃ³n esperada y criterios de evaluaciÃ³n.

---

<!-- ## âœ¨ CaracterÃ­sticas

**El sistema se desarrollo principalmente para**:

- Gestionar problemas acadÃ©micos con metadatos (tema, tipo, criterio, crÃ©ditosâ€¦) # Con gestionar nos referimos a crear, guardar, editar, etc...
- EvaluaciÃ³n automÃ¡tica de respuestas usando LLMs. # Usando los metadatos de los problemas como parte del contexto y las respuestas de los estudiantes se formula un prompt para que un LLM genere una evaluacion, la cual incluye una nota de 0 a 100, y una explicacion en caso de ser necesaria. -->

## ğŸ§  TecnologÃ­as

### ğŸ–¥ï¸ Frontend

- [Next.js](https://nextjs.org/)
- TypeScript
- TailwindCSS

> Estas tecnologÃ­as fueron seleccionadas por su eficiencia en el desarrollo de interfaces modernas, su ecosistema bien integrado y la posibilidad de desplegar fÃ¡cilmente la aplicaciÃ³n de forma gratuita en plataformas como [Vercel](https://vercel.com/).

### ğŸ§ª Backend

- [Flask](https://flask.palletsprojects.com/)
- Flask-SQLAlchemy
- Flask-Migrate
- [Fireworks API](https://nextjs.org/)
- Python 3.12+

> Se optÃ³ por estas herramientas debido a su simplicidad, rapidez de desarrollo y bajo overhead, ideales para un backend ligero centrado en una Ãºnica funcionalidad principal: evaluar respuestas mediante LLMs.

---

## ğŸ§  JustificaciÃ³n de herramientas

| Herramienta         | RazÃ³n de uso                                                                 |
|---------------------|------------------------------------------------------------------------------|
| **Flask**           | Microframework simple, ideal para construir APIs RESTful rÃ¡pidamente.        |
| **SQLAlchemy**      | ORM robusto para mantener independencia de la base de datos.                 |
| **Fireworks.ai**    | Proveedor de LLMs con buena calidad y facilidad de uso.                      |
| **Next.js**         | Framework moderno de React con SSR/SSG, ideal para apps rÃ¡pidas y SEO ready. |
| **TailwindCSS**     | Permite diseÃ±ar interfaces limpias y responsivas sin salir del HTML.         |
| **TypeScript**      | Mejora la mantenibilidad del frontend mediante tipado estÃ¡tico.              |

> Se optÃ³ por estas herramientas debido a su simplicidad, rapidez de desarrollo y bajo overhead, ideales para un backend ligero centrado en una Ãºnica funcionalidad principal: evaluar respuestas mediante LLMs.

---

## âš™ï¸ InstalaciÃ³n (ejemplo en Linux-Ubuntu)

### 1. Clona el repositorio

```bash
git clone https://github.com/ArcanoxXx-01/MMA_Project.git
cd MMA_Project/backend
```

### 2. Configura el Backend

```bash
python3 -m venv env
source env/bin/activate
pip install -r requirements.txt
```

Configura las variables de entorno en un archivo .env:

```bash
FIREWORKS_API_KEY=tu-api-key
```

Ejecuta la app:

```bash
python run.py
```

La API se levantarÃ¡ en <http://localhost:5000>.

### 3. Configura el Frontend

*duplica la terminal, y en la nueva ejecuta los siguientes comandos:

```bash
cd ../frontend
npm install
npm run dev
```

La app estarÃ¡ disponible en <http://localhost:3000>.

    AsegÃºrate de que el backend estÃ¡ corriendo en el puerto 5000 para que las peticiones funcionen correctamente.

---

## ğŸ” Estructura del proyecto

```bash
MMA_Project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ run.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx
â”‚   â”‚   â””â”€â”€ problems/
â”‚   â”‚       â”œâ”€â”€ [id]/page.tsx
â”‚   â”‚       â””â”€â”€ create/page.tsx
|   â”œâ”€â”€ public/images/
|   â”œâ”€â”€ src/
â”‚   |   â”œâ”€â”€ api/
|   |   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ types/
|   â”œâ”€â”€ package.json
|   â”œâ”€â”€ ...
â”‚   â””â”€â”€ tsconfig.json
â””â”€â”€ README.md
```

---

## ğŸ“ Arquitectura y DiseÃ±o

La aplicaciÃ³n sigue una arquitectura **desacoplada**, separando frontend y backend en mÃ³dulos independientes. Esto permite:

- Despliegue mÃ¡s flexible y escalable.
- Desarrollo paralelo entre equipo frontend y backend.
- Mejor mantenibilidad del cÃ³digo.

### ğŸ”„ ComunicaciÃ³n con LLMs

El backend no se comunica directamente con la API de Fireworks.ai, sino a travÃ©s de una **interfaz abstracta `BaseLLM`**, lo que permite:

- Sustituir Fireworks por otro proveedor (ej. OpenAI, Cohere, Mistral...) sin afectar el resto del cÃ³digo.
- Mantener una estructura uniforme para operaciones como `generate()` y `evaluate()`.
- Aislar detalles de implementaciÃ³n, lo que facilita pruebas, mantenibilidad y futura extensibilidad.

Actualmente, la clase `FireworksModel` implementa esta interfaz usando modelos **instructivos tipo chat**, ideales para tareas de evaluaciÃ³n textual guiadas por prompt.

---

## ğŸ§ª EvaluaciÃ³n con Fireworks.ai

La app se comunica con modelos LLM usando la API de Fireworks:

- El backend prepara un prompt detallado con:

  - Tema
  - Tipo de pregunta
  - SoluciÃ³n esperada
  - Respuesta del estudiante
  - Criterio de evaluaciÃ³n

- El modelo devuelve un razonamiento y una puntuaciÃ³n.

**Puedes ver o modificar el prompt en:**
`backend/app/llm/prompts.py`

---

## ğŸ“¥ Poblado de la base de datos (opcional)

Puedes usar un script `seed.py` en el backend para poblar la base de datos con ejemplos. AsegÃºrate de que estÃ© dentro del contexto de aplicaciÃ³n.

---

## ğŸ“Š AnÃ¡lisis del Proyecto

### âœ… Lo que funcionÃ³

- La **arquitectura desacoplada** entre frontend y backend facilitÃ³ el desarrollo modular, permitiendo trabajar de forma independiente y simplificando el despliegue y la mantenibilidad.
- El uso de una **interfaz abstracta (`BaseLLM`)** para la comunicaciÃ³n con modelos de lenguaje demostrÃ³ ser una decisiÃ³n acertada.
- El sistema **evalÃºa correctamente preguntas de tipo opciÃ³n mÃºltiple y verdadero/falso**, ya que estas no requieren razonamiento profundo.
- La interfaz web es intuitiva y funcional, con filtros, paginaciÃ³n y modo oscuro, lo que mejora la experiencia del usuario.

---

### âŒ Lo que no funcionÃ³ (aÃºn)

- Actualmente, el sistema **no guarda las evaluaciones generadas** por el modelo, lo que impide hacer trazabilidad, validaciones cruzadas o anÃ¡lisis posterior.
- **Las preguntas abiertas** (ej. justificar, explicar, demostrar) no siempre son bien evaluadas por el modelo.
- No se realizÃ³ comparaciÃ³n sistemÃ¡tica entre la evaluaciÃ³n automÃ¡tica y la evaluaciÃ³n humana.
- No se implementÃ³ un sistema de colas o tareas en segundo plano, lo cual puede afectar el rendimiento si se escalan las peticiones de evaluaciÃ³n.

---

## ğŸ› ï¸ Contribuciones y Futuro del Proyecto

Este proyecto estÃ¡ en evoluciÃ³n constante y hay mÃºltiples formas de contribuir o ampliarlo:

### ğŸ”§ Mejoras recomendadas

- **Persistencia de evaluaciones**: guardar los resultados generados por el modelo para permitir trazabilidad y anÃ¡lisis estadÃ­stico.
- **Historial por estudiante y por problema**: para observar la evoluciÃ³n individual y validar el comportamiento del modelo.
- **Afinar los prompts**: utilizando ejemplos (few-shot) y validaciÃ³n con respuestas reales para mejorar la calidad de las evaluaciones.
- **Agregar autenticaciÃ³n y control de acceso**: para gestionar roles de profesores, administradores o instituciones.
- **Soporte para mÃºltiples modelos**: integrar fÃ¡cilmente otros proveedores (OpenAI, Cohere, Mistralâ€¦) usando la interfaz `BaseLLM`.
- **ExportaciÃ³n y visualizaciÃ³n de resultados**: generar reportes en PDF, Excel u otros formatos para seguimiento acadÃ©mico.
- **Implementar feedback visual**: mostrar de forma clara al usuario el razonamiento del modelo, su puntuaciÃ³n y sugerencias.

### ğŸ¤ CÃ³mo contribuir

Â¡Tu ayuda es bienvenida! Algunas formas de colaborar:

- Agregar nuevos tipos de problemas y criterios de evaluaciÃ³n.
- Mejorar la interfaz web o la experiencia de usuario.
- Optimizar el rendimiento o refactorizar componentes.
- Proponer nuevas funcionalidades o integraciones.

---

## ğŸ§‘â€ğŸ’» Autor

DarÃ­o LÃ³pez FalcÃ³n â€” estudiante de Ciencias de la ComputaciÃ³n de la Universidad de La Habana

---

## ğŸ“„ Licencia

MIT License.
