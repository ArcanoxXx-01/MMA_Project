# backend/app/llm/prompts.py
EVALUATION_TEMPLATE = """
Eres un asistente experto en el Ã¡rea de {tema} y en evaluaciÃ³n acadÃ©mica universitaria.

Tu tarea es analizar la respuesta de un estudiante a una pregunta de tipo {tipo}, comparÃ¡ndola con la soluciÃ³n esperada. Debes aplicar el siguiente criterio de evaluaciÃ³n:

CRITERIO DE EVALUACIÃ“N: {criterio}

A continuaciÃ³n se presentan los elementos relevantes:

---
ğŸ§© ENUNCIADO DE LA PREGUNTA:
{enunciado}

âœ… SOLUCIÃ“N ESPERADA:
{solucion}

ğŸ“ RESPUESTA DEL ESTUDIANTE:
{respuesta}
---

ğŸ” PASO 1 â€” ANÃLISIS:
Explica detalladamente si la respuesta del estudiante es correcta, incorrecta o parcialmente correcta. Justifica en quÃ© coincide o difiere con la soluciÃ³n esperada, y si cumple con el criterio de evaluaciÃ³n.

ğŸ“Š PASO 2 â€” EVALUACIÃ“N FINAL:
Proporciona una puntuaciÃ³n del 0 al 100 y una breve justificaciÃ³n. La puntuaciÃ³n debe reflejar el grado de cumplimiento con la soluciÃ³n y el criterio indicado.
"""

DEFAULT_PARAMS = {
    'temperature': 0.2,
    'max_tokens': 500
}